# Assignment Instructions
Implement exact (variable elimination) and approximate (likelihood weighting, Gibbs sampling, Metropolis Hastings (MH)) methods for Bayesian networks (BNs).
#### Note on Metropolis Hastings implementation: 
There are a few plausible ways to interpet the MH sampling method as described in page 447 of the textbook.<br>
I suggest the most straightforward implementation: Given current sample, x, perform Gibbs sampling with probability p, else generate a random sample, x', with the WEIGHTED-SAMPLE algorithm. Samples generated by both the processes are always accepted. <br>
A possible alternate interpretation of the previous approach is the following. Always accept the next state generated by the Gibbs sampler. For x' generated with the WEIGHTED-SAMPLE algorithm, let the weight returned be π(x'). Also let π(x) be the weight of the current sample. Then, accept x' as the new state if π(x')>π(x), i.e., accept if the new state is more likely.<br>
In either case, you should report comparative results from experiments run with the following values for p: 0.75, 0.85, and 0.95.

#### Use the provided BN generator to create networks of different sizes, some of which are polytrees and others are not.
Investigate and report relative scale-up properties, by varying size of the BN, of different inference algorithms. Your report should provide details about
how you chose the data set to evaluate the scale-up properties, report run times including mean and standard deviations (use candlestick plots),
variation in converged probabilities (difference from the exact inference result on polytrees), relative effectiveness of working with downstream vs upstream evidence compared to query variables, relative effectiveness when CPT entries are close to/distant from 0/1 values.<br>

#### Note: 
You will need to include in your report a comparison of the accuracy of different approximation algorithms that you are implementing when compared with the variable elimination algorithm (exact inference). For this set of comparison experiments, use only polytree BNs. 

#### Code provided:
- Python code, developed by Robert Geraghty, for the enumeration-ask algorithm for exact inference in BNs
- Python code, developed by Chad Crawford, for generating BNs.

#### Timmy Flavin's observations and module code for generating polytrees ####
If the Probability argument is less than 1.0, the generator may generate a disconnected graph a.k.a. a poly forest. The probability of getting a poly forest goes up as the probability argument goes down but it's not like p=0.5 causes a 50% chance to make a forest. Set p to 1 to get trees or use the code included below.<br>
The generate tree graph draws arrows from child to parent instead of from parent to child.<br> 
Switch i and j so it is <br>
```g.add_edge(i, j) # replace this line```<br>
```g.add_edge(j, i) # with this line```<br>
I am unsure if the max_parents argument works correctly due to the arrows the wrong way
This code will work as a replacement for gen_polytree() if anyone wants it: 
```import random 
def tim_polytree(num_nodes, prob=0.5, max_parents=None):
  print(max_parents)
  edges = []
  clusters = []
  net = {}
  parent_net = {}
  # clusters start out as [[0],[1],[2],...,[num_nodes-1]]
  for i in range(num_nodes):
    clusters.append([i])
    net[i] = []
    parent_net[i] = []
  # While there are more than 2 clusters, randomly choose 2 
  # and 2 nodes from those and create an edge between them
  # if it does not violate the max parent rule
  while len(clusters) > 1:
    c1 = clusters.pop(random.randint(0,len(clusters)-1))
    c2 = clusters.pop(random.randint(0,len(clusters)-1))
    e1 = c1[random.randint(0,len(c1)-1)] #source
    e2 = c2[random.randint(0,len(c2)-1)] #sink
    if len(net[e2]) < max_parents:
      edges.append([e1,e2])
      net[e2].append(e1)
      parent_net[e1].append(e2) # keeps list of this node's children
      clusters.append(c1+c2)
    else:
      clusters.append(c1)
      clusters.append(c2)
  print(net)
  print(parent_net)
  return net```