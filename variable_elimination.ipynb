{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Variable Elimination to approximate probability queries from a Bayesian Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to to this, ultimately we'll need to use dijoint sets as certain factors will be joined into other factors\n",
    "class DisjointSet:\n",
    "    def __init__(self):\n",
    "        self.__nodes = {}\n",
    "\n",
    "    class __Node:\n",
    "        def __init__(self, id: int):\n",
    "            self.__id = id\n",
    "            self.__parent = self\n",
    "\n",
    "        def __compress(self):\n",
    "            if self.__parent != self:\n",
    "                self.__parent.__compress()\n",
    "                self.__parent = self.__parent.__parent\n",
    "        \n",
    "        def join(self, other):\n",
    "            self.__compress()\n",
    "            other.__compress()\n",
    "            if self.__parent != self:\n",
    "                self.__parent.join(other)\n",
    "            elif other.__parent != other:\n",
    "                self.join(other.__parent)\n",
    "            else:\n",
    "                # both are parent nodes\n",
    "                self.__parent = other\n",
    "            self.__compress()\n",
    "            other.__compress()\n",
    "\n",
    "        def get_parent(self) -> int:\n",
    "            return self.__parent.__id\n",
    "    \n",
    "    def add_element(self, id: int):\n",
    "        if id not in self.__nodes.keys():\n",
    "            self.__nodes[id] = DisjointSet.__Node(id=id)\n",
    "    \n",
    "    def join(self, first_id: int, second_id: int):\n",
    "        if first_id in self.__nodes.keys() and second_id in self.__nodes.keys():\n",
    "            self.__nodes[first_id].join(self.__nodes[second_id])\n",
    "    \n",
    "    def get(self, id: int) -> int:\n",
    "        return self.__nodes[id].get_parent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_row_sum_pairs(var: int, all_vars: list[int]) -> list[tuple[int,int]]:\n",
    "    \"\"\"Given a variable that should be eliminated by summing over its possibilities, and the ordered list of all variables, find all of the pairs of rows that should be summed together to eliminate said variable\n",
    "\n",
    "    Args:\n",
    "        var (int): variable to eliminate\n",
    "        all_vars (list[int]): ordered list of all variables\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[int,int]]: list of pairs of rows that should be summed together in the corresponding array of values\n",
    "    \"\"\"\n",
    "    idx = all_vars.index(var)\n",
    "    binary_posn = len(all_vars) - 1 - idx\n",
    "    row_pairs = []\n",
    "    prev_second = 1\n",
    "    for i in range(2**(len(all_vars)-1)):\n",
    "        # this is how many row pairs there will be - all other possible combinations of all other variables\n",
    "        first = i if i < prev_second else prev_second+1\n",
    "        second = first + 2**binary_posn\n",
    "        prev_second = second\n",
    "        row_pairs.append([first,second])\n",
    "    return row_pairs\n",
    "\n",
    "def find_corresponding_rows(bit_mask: list[int], common_vars: list[int], all_vars: list[int]) -> list[int]:\n",
    "    \"\"\"Given a list of common variables and which ones are set to true, find the rows in the distribution array that would correspond to all_vars which share the same truth value\n",
    "\n",
    "    Args:\n",
    "        bit_mask (list[int]): truth values for each of the common variables\n",
    "        common_vars (list[int]): list of common variables\n",
    "        all_vars (list[int]): list of all variables (common variables will be a subset)\n",
    "\n",
    "    Returns:\n",
    "        list[int]: list of rows corresponding with the truth values associated with the common variables\n",
    "    \"\"\"\n",
    "    vars_to_posn = {v : all_vars.index(v) for v in common_vars}\n",
    "    common_rows = set()\n",
    "    # we will have a list of sets which we will take the intersection of\n",
    "    row_sets = [set() for _ in common_vars]\n",
    "    for i, v in enumerate(common_vars):\n",
    "        idx = vars_to_posn[v]\n",
    "        binary_posn = len(all_vars) - 1 - idx\n",
    "        switch_every = 2 ** binary_posn\n",
    "        # see if at this row, the given common variable's truth value is matched\n",
    "        for row in range(2**len(all_vars)):\n",
    "            negative_row = ((row // switch_every) % 2) == 0\n",
    "            if not bit_mask[i] and negative_row:\n",
    "                row_sets[i].add(row)\n",
    "            elif bit_mask[i] and not negative_row:\n",
    "                row_sets[i].add(row)\n",
    "    # now take the intersection of all the row sets\n",
    "    for row in row_sets[0]:\n",
    "        missing = False\n",
    "        for i in range(1, len(row_sets)):\n",
    "            if row not in row_sets[i]:\n",
    "                missing = True\n",
    "                break\n",
    "        if not missing:\n",
    "            common_rows.add(row)\n",
    "\n",
    "    result = list(common_rows)\n",
    "    result.sort()\n",
    "    return result\n",
    "\n",
    "def is_subset(first: set[int], second: set[int]) -> bool:\n",
    "    \"\"\"Return if the first set is a subset of the second set\n",
    "\n",
    "    Args:\n",
    "        first (set[int]): supposed subset\n",
    "        second (set[int]): supposed superset\n",
    "\n",
    "    Returns:\n",
    "        bool: whether the first set is a subset of the second set\n",
    "    \"\"\"\n",
    "    for v in first:\n",
    "        if v not in second:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_common_rows(prev_factor_vars: list[int], next_factor_vars: list[int]) -> tuple[dict[int,list[int]],list[int]]:\n",
    "    \"\"\"Given two (ordered) lists of variables, determine which rows of the second factor must be multiplied by each row of the first factor\n",
    "\n",
    "    Args:\n",
    "        prev_factor_vars (list[int]): variables corresponding to first factor/distribution\n",
    "        next_factor_vars (list[int]): variables corresponding to second factor/distribution\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int,list[int]],list[int]]: for each row of the first factor, which rows of the second factor should it multiply? Also, return the variable order for the joined array.\n",
    "    \"\"\"\n",
    "    common_vars = [v for v in prev_factor_vars if v in next_factor_vars]\n",
    "    common_vars_set = set(common_vars)\n",
    "    \n",
    "    # Each row in the previous factor corresponding to one combination of the values for the variables in common must multiply each row in the second factor \n",
    "    row_multiplications = {}\n",
    "    # Calculate multiplication consistencies\n",
    "    for i in range(2**len(common_vars)):\n",
    "        # i represents the bit mask for all the binary values of the common variables\n",
    "        # for each appearance of these shared variables, all such rows in the first factor multiply by all such rows in the second factor\n",
    "        bit_mask = [0 for _ in range(len(common_vars))]\n",
    "        for j in range(len(common_vars)):\n",
    "            # see which variables are set to true by creating the bit mask\n",
    "            if ((1 << j) & i) > 0:\n",
    "                bit_mask[j] += 1\n",
    "        # now we have a bit mask telling us which variables will be true - find the rows for each of the two factors where this combination of truth values for the shared variables occurs\n",
    "        prev_factor_rows = find_corresponding_rows(bit_mask, common_vars, prev_factor_vars)\n",
    "        next_factor_rows = find_corresponding_rows(bit_mask, common_vars, next_factor_vars)\n",
    "        for prev_row in prev_factor_rows:\n",
    "            row_multiplications[prev_row] = []\n",
    "            for next_row in next_factor_rows:\n",
    "                row_multiplications[prev_row].append(next_row)\n",
    "\n",
    "    # TODO - fix this joined variable order...\n",
    "    prev_vars = set(prev_factor_vars)\n",
    "    next_vars = set(next_factor_vars)\n",
    "    if is_subset(prev_vars, next_vars):\n",
    "        joined_vars = next_factor_vars\n",
    "    elif is_subset(next_vars, prev_vars):\n",
    "        joined_vars = prev_factor_vars\n",
    "    else:\n",
    "        joined_vars = [v for v in prev_factor_vars] + [v for v in next_factor_vars if v not in common_vars_set]\n",
    "    return (row_multiplications, joined_vars)\n",
    "\n",
    "def join_factors(var: int, eliminate: bool, relevant_factors: list[tuple[list[int],np.array]]) -> tuple[list[int],np.array]:\n",
    "    \"\"\"Given which variable we want to eliminate, multiply its respective factors together and then sum out over the variable to eliminate\n",
    "\n",
    "    Args:\n",
    "        var (int): the variable (possibly to eliminate)\n",
    "        eliminate (bool): whether said variable will be eliminated\n",
    "        factors (list[tuple[list[int],np.array]]): list of distributions corresponding to this variable\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[int],np.array]: new list of relevant variables to this array along with the array (a new distribution)\n",
    "    \"\"\"\n",
    "    # We need to find out which rows multiply together\n",
    "    # Break into pairs\n",
    "    prev_factor = relevant_factors[0]\n",
    "    for i in range(1, len(relevant_factors)):\n",
    "        next_factor = relevant_factors[i]\n",
    "        # Find the list of rows corresponding to each factor that must multiply together\n",
    "        row_multiplications, joined_vars = find_common_rows(prev_factor[0], next_factor[0])\n",
    "        # Create a new array to populate with products\n",
    "        merged_array = np.zeros(shape=2**len(joined_vars))\n",
    "        for prev_idx in row_multiplications:\n",
    "            for next_idx in row_multiplications[prev_idx]:\n",
    "                # TODO - calculate curr_idx based on prev_idx and next_idx\n",
    "                merged_array[next_idx] = prev_factor[1][prev_idx] * next_factor[1][next_idx]\n",
    "        # Set up for the next two multiplications\n",
    "        merged = (joined_vars, merged_array)\n",
    "        prev_factor = merged\n",
    "\n",
    "    # If eliminating, sum over the variable to eliminate it\n",
    "    resulting_variables = prev_factor[0]\n",
    "    resulting_array = prev_factor[1]\n",
    "    summed_array = np.zeros(len(resulting_array)//2) if eliminate else resulting_array\n",
    "    if eliminate:\n",
    "        # Find the corresponding pairs of rows that must be summed together\n",
    "        row_pairs = find_row_sum_pairs(var, resulting_variables)\n",
    "        for i, row_pair in enumerate(row_pairs):\n",
    "            summed_array[i] = resulting_array[row_pair[0]] + resulting_array[row_pair[1]]\n",
    "\n",
    "    return [v for v in resulting_variables if (v != var if eliminate else True)], summed_array\n",
    "\n",
    "\n",
    "def relevant(parents: list[int], parents_in_evidence: list[int], evidence: dict[int,bool], row: int) -> bool:\n",
    "    \"\"\"Based on the parents' order for a given probability array, which parents are in the evidence, and whether those parents are true or false, determine if the row is consistent with the evidence based on our probability array construction convention\n",
    "\n",
    "    Args:\n",
    "        parents (list[int]): list of parents in order for some probability array\n",
    "        parents_in_evidence (list[int]): list of parents which are in the evidence\n",
    "        evidence (dict[int,bool]): records which variables in the evidence are true or false\n",
    "        row (int): row index to determine if consistent with evidence\n",
    "\n",
    "    Returns:\n",
    "        bool: whether row number is consistent with evidence\n",
    "    \"\"\"\n",
    "    for parent in parents_in_evidence:\n",
    "        parent_idx = parents.index(parent)\n",
    "        binary_posn = len(parents) - 1 - parent_idx\n",
    "        switch_every = 2 ** binary_posn\n",
    "        negative_row = ((row // switch_every) % 2) == 0\n",
    "        if (negative_row and evidence[parent]) or ((not negative_row) and (not evidence[parent])):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def handle_vars(vars: list[int], eliminate: bool, factor_index_to_factor: dict[int,tuple[list[int],np.array]], factor_tracker: DisjointSet, var_to_factor_indices: dict[int,list[int]]) -> np.array:\n",
    "    \"\"\"Join the list of factors, with a flag to determine if said variables are to be eliminated\n",
    "\n",
    "    Args:\n",
    "        vars (list[int]): list of variables\n",
    "        eliminate (bool): whether said variables are to be eliminated\n",
    "        factor_index_to_factor (dict[int,tuple[list[int],np.array]]): mapping factor indices to the factors themselves\n",
    "        factor_tracker (DisjointSet): to handle joining of merged factors by index\n",
    "        var_to_factor_indices (dict[int,list[int]]): mapping variables to list of relevant factor indices that include said variable\n",
    "\n",
    "    Returns:\n",
    "        np.array: final distribution of numbers\n",
    "    \"\"\"\n",
    "    array = None\n",
    "    for var in vars:\n",
    "        unique_factor_indices = set([factor_tracker.get(f_idx) for f_idx in var_to_factor_indices[var]])\n",
    "        unique_factors = [factor_index_to_factor[f_idx] for f_idx in unique_factor_indices]\n",
    "        resulting_vars, array = join_factors(var=var, eliminate=eliminate, relevant_factors=unique_factors)\n",
    "        join_base = None\n",
    "        # Factors are now merged - so update factor tracker accordingly\n",
    "        for f_idx in unique_factor_indices:\n",
    "            if join_base == None:\n",
    "                join_base = f_idx\n",
    "            factor_index_to_factor[f_idx] = (resulting_vars, array)\n",
    "            factor_tracker.join(f_idx, join_base)\n",
    "    return array\n",
    "\n",
    "def return_query_probabilities(queries: list[int], evidence: dict[int,bool], network: dict) -> np.array:\n",
    "    \"\"\"Given a Bayesian network and a list of query and evidence variables, return the probability distribution for all possible values of query variables\n",
    "\n",
    "    Args:\n",
    "        queries (list[int]): list of variables specified whose value probabilities we want to query\n",
    "        evidence (list[tuple[int,bool]]): list of variables whose values are specified and hence affect query probabilities\n",
    "        network (dict): underlying network which reveals probabilities of each node given its parents' values\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: probability distribution of possible combination values of each of the query variables (2^{#query variables}, 0 is all false and 2^{#query variables}-1 is all true)\n",
    "    \"\"\"\n",
    "    # each factor has its own probability distribution - which will depend on its parents should they exist\n",
    "    factor_index_to_factor = {}\n",
    "    var_to_factor_indices = {i: set() for i in range(len(network))}\n",
    "    for i, info in network.items():\n",
    "        i = int(i)\n",
    "\n",
    "        parents = info[\"parents\"]\n",
    "        parents_in_evidence = [p for p in parents if p in evidence.keys()]\n",
    "        parents_not_in_evidence = [p for p in parents if p not in evidence.keys()]\n",
    "        # obtain the array of probabilities before considering evidence\n",
    "        prob_array = np.array([pair[1] for pair in info[\"prob\"]])\n",
    "\n",
    "        # now filter the probability array so that it only contains entries consistent with the evidence\n",
    "        relevant_rows = []\n",
    "        for row in range(len(prob_array)):\n",
    "            if relevant(parents, parents_in_evidence, evidence, row):\n",
    "                relevant_rows.append(row)\n",
    "        # quick sanity check\n",
    "        assert len(relevant_rows) == 2 ** len(parents_not_in_evidence)\n",
    "\n",
    "        # our base array size depends on the number of parents not in our evidence (and so take on both true and false values)\n",
    "        new_prob_array = np.zeros(len(relevant_rows))\n",
    "        # fill new_prob_array with relevant rows\n",
    "        for j, row in enumerate(relevant_rows):\n",
    "            new_prob_array[j] = prob_array[row]\n",
    "\n",
    "        # double the size of our array so that it can contain the probabilities of 'i' being true and false\n",
    "        # BUT only if we are not in the evidence\n",
    "        final_prob_array = np.zeros((2 if i not in evidence.keys() else 1) * len(new_prob_array))\n",
    "        for j in range(len(prob_array)):\n",
    "            if i not in evidence.keys():\n",
    "                final_prob_array[2*j] = 1 - new_prob_array[j]\n",
    "                final_prob_array[2*j+1] = new_prob_array[j]\n",
    "            elif evidence[i]:\n",
    "                # i is set to true in evidence\n",
    "                final_prob_array[j] = new_prob_array[j]\n",
    "            else:\n",
    "                # i is set to false in evidence\n",
    "                final_prob_array[j] = 1 - new_prob_array[j]\n",
    "\n",
    "        # finally store into a tuple - and only count 'i' as one of the variables if it is not in the evidence\n",
    "        prob_tuple = ([parent for parent in parents] + ([i] if i not in evidence.keys() else []), final_prob_array)\n",
    "        for var in prob_tuple[0]:\n",
    "            var_to_factor_indices[var].add(len(factor_index_to_factor))\n",
    "        factor_index_to_factor[len(factor_index_to_factor)] = prob_tuple\n",
    "\n",
    "    # ultimately, factors will be merged, and thus their index will correspond to the same set variable - we do not want that showing up multiple times when we consider the relevant factors to a variable below\n",
    "    factor_tracker = DisjointSet()\n",
    "    for i in range(len(factor_index_to_factor)):\n",
    "        factor_tracker.add_element(id=i)\n",
    "\n",
    "    # figure out which variables need to be eliminated\n",
    "    query_set = set(queries)\n",
    "    evidence_set = set([pair[0] for pair in evidence.items()])\n",
    "    hidden_vars = [i for i in range(len(network)) if i not in query_set and i not in evidence_set]\n",
    "    # sort the hidden variables by the number of relevant factors\n",
    "    hidden_vars.sort(key=lambda x : -len(var_to_factor_indices[x]))\n",
    "\n",
    "    # now we go through and eliminate each hidden variable\n",
    "    handle_vars(vars=hidden_vars, eliminate=True, factor_index_to_factor=factor_index_to_factor, factor_tracker=factor_tracker,var_to_factor_indices=var_to_factor_indices)\n",
    "    result = handle_vars(vars=queries, eliminate=False, factor_index_to_factor=factor_index_to_factor, factor_tracker=factor_tracker,var_to_factor_indices=var_to_factor_indices) # this function also returns a factor\n",
    "    return result / np.sum(result) # for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34953879 0.056898   0.10097538 0.49258784]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "queries = [0, 3]\n",
    "evidence = {2:True}\n",
    "\n",
    "with open('bn_test_2.json') as f:\n",
    "    bayesian_network = json.load(f)\n",
    "    print(return_query_probabilities(queries, evidence, bayesian_network))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
