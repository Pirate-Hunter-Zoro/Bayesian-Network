{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Variable Elimination to approximate probability queries from a Bayesian Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an arbitrary Bayesian Network, an arbitrary set of query variables, and arbitrary evidence, we want to be able to calculate the probability of said variable...\n",
    "import numpy as np\n",
    "\n",
    "def join(first: np.array, second: np.array, row_pairings: dict[int,list[int]]) -> np.array:\n",
    "    \"\"\"Given two distributions and the variable to to join on, join the two distributions into a third\n",
    "\n",
    "    Args:\n",
    "        first (np.ndarray): first distribution (smaller or same size)\n",
    "        second (np.ndarray): second distribution (larger or same size)\n",
    "        row_pairings (dict[int,list[int]]): map of rows of smaller first distribution mapping to rows of second distribution they will multiply\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: resulting distribution after joining\n",
    "    \"\"\"\n",
    "    result = np.zeros(second.shape)\n",
    "    for first_index, second_indices in row_pairings.items():\n",
    "        for s in second_indices:\n",
    "            result[s] += first[first_index] * second[s]\n",
    "    return result\n",
    "\n",
    "def eliminate(distribution: np.array, pairings: list[tuple[int,int]]) -> np.array:\n",
    "    \"\"\"eliminate a variable by summing over the distribution at the specified index pairs\n",
    "\n",
    "    Args:\n",
    "        distribution (np.ndarray): pairs of indices we add together\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: resulting distribution (dimensions will be half as large)\n",
    "    \"\"\"\n",
    "    result = np.array(shape=len(pairings))\n",
    "    for i, pair in enumerate(pairings):\n",
    "        result[i] = distribution[pair[0]] + distribution[pair[1]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_1 = np.array([.000999, .00029, .93906, .00095])\n",
    "distr_2 = np.array([.95,.05,.55,.45,.17,.83,.23,.77])\n",
    "joined = join(distr_1, distr_2, {0:[0,1],1:[2,3],2:[4,5],3:[6,7]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "array() got an unexpected keyword argument 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meliminate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjoined\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpairings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36meliminate\u001b[0;34m(distribution, pairings)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21meliminate\u001b[39m(distribution: np\u001b[38;5;241m.\u001b[39marray, pairings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m,\u001b[38;5;28mint\u001b[39m]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39marray:\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"eliminate a variable by summing over the distribution at the specified index pairs\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m        np.ndarray: resulting distribution (dimensions will be half as large)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpairings\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, pair \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pairings):\n\u001b[1;32m     32\u001b[0m         result[i] \u001b[38;5;241m=\u001b[39m distribution[pair[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m+\u001b[39m distribution[pair[\u001b[38;5;241m1\u001b[39m]]\n",
      "\u001b[0;31mTypeError\u001b[0m: array() got an unexpected keyword argument 'shape'"
     ]
    }
   ],
   "source": [
    "eliminate(joined, pairings=[(0,2),(1,3),(4,6),(5,7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def find_row_sum_pairs(var: int, all_vars: list[int]) -> list[tuple[int,int]]:\n",
    "    \"\"\"Given a variable that should be eliminated by summing over its possibilities, and the ordered list of all variables, find all of the pairs of rows that should be summed together to eliminate said variable\n",
    "\n",
    "    Args:\n",
    "        var (int): variable to eliminate\n",
    "        all_vars (list[int]): ordered list of all variables\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[int,int]]: list of pairs of rows that should be summed together in the corresponding array of values\n",
    "    \"\"\"\n",
    "    idx = all_vars.index(var)\n",
    "    binary_posn = len(all_vars) - 1 - idx\n",
    "    row_pairs = []\n",
    "    prev_second = 1\n",
    "    for i in range(2**(len(all_vars)-1)):\n",
    "        # this is how many row pairs there will be - all other possible combinations of all other variables\n",
    "        first = i if i < prev_second else prev_second+1\n",
    "        second = first + 2**binary_posn\n",
    "        prev_second = second\n",
    "        row_pairs.append([first,second])\n",
    "    return row_pairs\n",
    "\n",
    "def find_corresponding_rows(bit_mask: list[int], common_vars: list[int], all_vars: list[int]) -> list[int]:\n",
    "    \"\"\"Given a list of common variables and which ones are set to true, find the rows in the distribution array that would correspond to all_vars which share the same truth value\n",
    "\n",
    "    Args:\n",
    "        bit_mask (list[int]): truth values for each of the common variables\n",
    "        common_vars (list[int]): list of common variables\n",
    "        all_vars (list[int]): list of all variables (common variables will be a subset)\n",
    "\n",
    "    Returns:\n",
    "        list[int]: list of rows corresponding with the truth values associated with the common variables\n",
    "    \"\"\"\n",
    "    vars_to_posn = {v : all_vars.index(v) for v in common_vars}\n",
    "    common_rows = set()\n",
    "    # we will have a list of sets which we will take the intersection of\n",
    "    row_sets = [set() for _ in common_vars]\n",
    "    for i, v in enumerate(common_vars):\n",
    "        idx = vars_to_posn[v]\n",
    "        binary_posn = len(all_vars) - 1 - idx\n",
    "        switch_every = 2 ** binary_posn\n",
    "        # see if at this row, the given common variable's truth value is matched\n",
    "        for row in range(2**len(all_vars)):\n",
    "            negative_row = ((row // switch_every) % 2) == 0\n",
    "            if not bit_mask[i] and negative_row:\n",
    "                row_sets[i].add(row)\n",
    "            elif bit_mask[i] and not negative_row:\n",
    "                row_sets[i].add(row)\n",
    "    # now take the intersection of all the row sets\n",
    "    for row in row_sets[0]:\n",
    "        missing = False\n",
    "        for i in range(1, len(row_sets)):\n",
    "            if row not in row_sets[i]:\n",
    "                missing = True\n",
    "                break\n",
    "        if not missing:\n",
    "            common_rows.add(row)\n",
    "\n",
    "    result = list(common_rows)\n",
    "    result.sort()\n",
    "    return result\n",
    "\n",
    "def find_common_rows(prev_factor_vars: list[int], next_factor_vars: list[int]) -> tuple[dict[int,list[int]],list[int]]:\n",
    "    \"\"\"Given two (ordered) lists of variables, determine which rows of the second factor must be multiplied by each row of the first factor\n",
    "\n",
    "    Args:\n",
    "        prev_factor_vars (list[int]): variables corresponding to first factor/distribution\n",
    "        next_factor_vars (list[int]): variables corresponding to second factor/distribution\n",
    "\n",
    "    Returns:\n",
    "        tuple[dict[int,list[int]],list[int]]: for each row of the first factor, which rows of the second factor should it multiply? Also, return the variable order for the joined array.\n",
    "    \"\"\"\n",
    "    common_vars = [v for v in prev_factor_vars if v in next_factor_vars]\n",
    "    common_vars_set = set(common_vars)\n",
    "    \n",
    "    # Each row in the previous factor corresponding to one combination of the values for the variables in common must multiply each row in the second factor \n",
    "    row_multiplications = {}\n",
    "    # Calculate multiplication consistencies\n",
    "    for i in range(2**len(common_vars)):\n",
    "        # i represents the bit mask for all the binary values of the common variables\n",
    "        # for each appearance of these shared variables, all such rows in the first factor multiply by all such rows in the second factor\n",
    "        bit_mask = [0 for _ in range(len(common_vars))]\n",
    "        for j in range(len(common_vars)):\n",
    "            # see which variables are set to true by creating the bit mask\n",
    "            if ((1 << j) & i) > 0:\n",
    "                bit_mask[j] += 1\n",
    "        # now we have a bit mask telling us which variables will be true - find the rows for each of the two factors where this combination of truth values for the shared variables occurs\n",
    "        prev_factor_rows = find_corresponding_rows(bit_mask, common_vars, prev_factor_vars)\n",
    "        next_factor_rows = find_corresponding_rows(bit_mask, common_vars, next_factor_vars)\n",
    "        for prev_row in prev_factor_rows:\n",
    "            row_multiplications[prev_row] = []\n",
    "            for next_row in next_factor_rows:\n",
    "                row_multiplications[prev_row].append(next_row)\n",
    "\n",
    "    joined_vars = [v for v in prev_factor_vars] + [v for v in next_factor_vars if v not in common_vars_set]\n",
    "    return (row_multiplications, joined_vars)\n",
    "\n",
    "def join_factors(var: int, eliminate: bool, relevant_factors: list[tuple[list[int],np.array]]) -> tuple[list[int],np.array]:\n",
    "    \"\"\"Given which variable we want to eliminate, multiply its respective factors together and then sum out over the variable to eliminate\n",
    "\n",
    "    Args:\n",
    "        var (int): the variable (possibly to eliminate)\n",
    "        eliminate (bool): whether said variable will be eliminated\n",
    "        factors (list[tuple[list[int],np.array]]): list of distributions corresponding to this variable\n",
    "\n",
    "    Returns:\n",
    "        tuple[list[int],np.array]: new list of relevant variables to this array along with the array (a new distribution)\n",
    "    \"\"\"\n",
    "    # We need to find out which rows multiply together\n",
    "    # Break into pairs\n",
    "    prev_factor = relevant_factors[0]\n",
    "    for i in range(1, len(relevant_factors)):\n",
    "        next_factor = relevant_factors[i]\n",
    "        # Find the list of rows corresponding to each factor that must multiply together\n",
    "        row_multiplications, joined_vars = find_common_rows(prev_factor[0], next_factor[0])\n",
    "        # Create a new array to populate with products\n",
    "        merged_array = np.zeros(shape=2**len(joined_vars))\n",
    "        current_idx = 0\n",
    "        for prev_idx in row_multiplications.keys():\n",
    "            for next_idx in row_multiplications[prev_idx]:\n",
    "                merged_array[current_idx] = prev_factor[1][prev_idx] * next_factor[1][next_idx]\n",
    "                current_idx += 1\n",
    "        # Set up for the next two multiplications\n",
    "        merged = (joined_vars, merged_array)\n",
    "        prev_factor = merged\n",
    "\n",
    "    # If eliminating, sum over the variable to eliminate it\n",
    "    resulting_variables = prev_factor[0]\n",
    "    resulting_array = prev_factor[1]\n",
    "    summed_array = np.zeros(len(resulting_array)//2) if eliminate else resulting_array\n",
    "    if eliminate:\n",
    "        # Find the corresponding pairs of rows that must be summed together\n",
    "        row_pairs = find_row_sum_pairs(var, resulting_variables)\n",
    "        for i, row_pair in enumerate(row_pairs):\n",
    "            summed_array[i] = resulting_array[row_pair[0]] + resulting_array[row_pair[1]]\n",
    "\n",
    "    return [v for v in resulting_variables if (v != var if eliminate else True)], summed_array\n",
    "\n",
    "\n",
    "def relevant(parents: list[int], parents_in_evidence: list[int], evidence: dict[int,bool], row: int) -> bool:\n",
    "    \"\"\"Based on the parents' order for a given probability array, which parents are in the evidence, and whether those parents are true or false, determine if the row is consistent with the evidence based on our probability array construction convention\n",
    "\n",
    "    Args:\n",
    "        parents (list[int]): list of parents in order for some probability array\n",
    "        parents_in_evidence (list[int]): list of parents which are in the evidence\n",
    "        evidence (dict[int,bool]): records which variables in the evidence are true or false\n",
    "        row (int): row index to determine if consistent with evidence\n",
    "\n",
    "    Returns:\n",
    "        bool: whether row number is consistent with evidence\n",
    "    \"\"\"\n",
    "    for parent in parents_in_evidence:\n",
    "        parent_idx = parents.index(parent)\n",
    "        binary_posn = len(parents) - 1 - parent_idx\n",
    "        switch_every = 2 ** binary_posn\n",
    "        negative_row = ((row // switch_every) % 2) == 0\n",
    "        if (negative_row and evidence[parent]) or ((not negative_row) and (not evidence[parent])):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def return_query_probabilities(queries: list[int], evidence: dict[int,bool], network: dict) -> np.array:\n",
    "    \"\"\"Given a Bayesian network and a list of query and evidence variables, return the probability distribution for all possible values of query variables\n",
    "\n",
    "    Args:\n",
    "        queries (list[int]): list of variables specified whose value probabilities we want to query\n",
    "        evidence (list[tuple[int,bool]]): list of variables whose values are specified and hence affect query probabilities\n",
    "        network (dict): underlying network which reveals probabilities of each node given its parents' values\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: probability distribution of possible combination values of each of the query variables (2^{#query variables}, 0 is all false and 2^{#query variables}-1 is all true)\n",
    "    \"\"\"\n",
    "    # we need a map of NON-EVIDENCE variables to their respective factors (some of which will be shared) - factors are integers serving as references to numpy arrays\n",
    "    factor_mappings = {int(i):[] for i in network.keys()}\n",
    "    factors = {}\n",
    "    # each factor has its own probability distribution - which may or may not depend on its parents\n",
    "    for i, info in network.items():\n",
    "        i = int(i)\n",
    "\n",
    "        parents = info[\"parents\"]\n",
    "        parents_in_evidence = [p for p in parents if p in evidence.keys()]\n",
    "        parents_not_in_evidence = [p for p in parents if p not in evidence.keys()]\n",
    "        # obtain the array of probabilities before considering evidence\n",
    "        prob_array = np.array([pair[1] for pair in info[\"prob\"]])\n",
    "\n",
    "        # now filter the probability array so that it only contains entries consistent with the evidence\n",
    "        relevant_rows = []\n",
    "        for row in range(len(prob_array)):\n",
    "            if relevant(parents, parents_in_evidence, evidence, row):\n",
    "                relevant_rows.append(row)\n",
    "        # quick sanity check\n",
    "        assert len(relevant_rows) == 2 ** len(parents_not_in_evidence)\n",
    "\n",
    "        # our base array size depends on the number of parents not in our evidence (and so take on both true and false values)\n",
    "        new_prob_array = np.zeros(len(relevant_rows))\n",
    "        # fill new_prob_array with relevant rows\n",
    "        for j, row in enumerate(relevant_rows):\n",
    "            new_prob_array[j] = prob_array[row]\n",
    "\n",
    "        # double the size of our array so that it can contain the probabilities of 'i' being true and false\n",
    "        # BUT only if we are not in the evidence\n",
    "        final_prob_array = np.zeros((2 if i not in evidence.keys() else 1) * len(new_prob_array))\n",
    "        for j in range(len(prob_array)):\n",
    "            if i not in evidence.keys():\n",
    "                final_prob_array[2*j] = 1 - new_prob_array[j]\n",
    "                final_prob_array[2*j+1] = new_prob_array[j]\n",
    "            elif evidence[i]:\n",
    "                # i is set to true in evidence\n",
    "                final_prob_array[j] = new_prob_array[j]\n",
    "            else:\n",
    "                # i is set to false in evidence\n",
    "                final_prob_array[j] = 1 - new_prob_array[j]\n",
    "\n",
    "        # finally store into a tuple - and only count 'i' as one of the variables if it is not in the evidence\n",
    "        prob_tuple = ([parent for parent in parents] + ([i] if i not in evidence.keys() else []), final_prob_array)\n",
    "        # each involved variable needs THAT reference to prob_tuple\n",
    "        if i not in evidence.keys():\n",
    "            factor_mappings[i].append(len(factors))\n",
    "        for p in parents:\n",
    "            if p not in evidence.keys():\n",
    "                factor_mappings[p].append(len(factors))\n",
    "        factors[len(factors)] = prob_tuple\n",
    "    \n",
    "    # figure out which variables need to be eliminated\n",
    "    query_set = set(queries)\n",
    "    evidence_set = set([pair[0] for pair in evidence.items()])\n",
    "    hidden_vars = [i for i in range(len(network)) if i not in query_set and i not in evidence_set]\n",
    "    # eliminate the hidden variables in an order based on their number of factors - more factors means eliminate sooner\n",
    "    hidden_vars.sort(key=lambda i : -len(factor_mappings[i]))\n",
    "\n",
    "    # now we go through and eliminate each variable\n",
    "    for var in hidden_vars:\n",
    "        resulting_vars, array = join_factors(var=var, eliminate=True, relevant_factors=[factors[i] for i in factor_mappings[var]])\n",
    "        for i in factor_mappings[var]:\n",
    "            factors[i] = (resulting_vars, array)\n",
    "\n",
    "    # now go through the query variables\n",
    "    for var in queries:\n",
    "        resulting_vars, array = join_factors(var=var, eliminate=False, relevant_factors=[factors[i] for i in factor_mappings[var]])\n",
    "        for i in factor_mappings[var]:\n",
    "            factors[i] = (resulting_vars, array)\n",
    "\n",
    "    # now all factor keys for the dictionary should have the same reduced array of probability values\n",
    "    result = factors[0][1]\n",
    "    return result / np.sum(result) # for normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.07174842e-02 4.52096402e-10 9.59282513e-01 2.35105063e-09]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "queries = [0, 3]\n",
    "evidence = {2:True}\n",
    "\n",
    "with open('bn_test_2.json') as f:\n",
    "    bayesian_network = json.load(f)\n",
    "    print(return_query_probabilities(queries, evidence, bayesian_network))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
